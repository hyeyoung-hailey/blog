## **1. ê°œìš”**
ì´ ê¸€ì—ì„œëŠ” **Kafka, MySQL, Docker, Debezium, Elasticsearch, Kibana**ë¥¼ í™œìš©í•´ **CDC(ë°ì´í„° ë³€ê²½ ì‚¬í•­ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€í•˜ê³  ë°˜ì˜í•˜ëŠ”)ë¥¼ êµ¬í˜„**í•˜ê³ , ë³€ê²½ëœ ë°ì´í„°ë¥¼ **Elasticsearchì— ì €ì¥í•˜ì—¬ ê²€ìƒ‰ ë° ì‹œê°í™”**í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## **2. Elasticsearch & Kibana ì†Œê°œ**
- **Elasticsearch**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ê²€ìƒ‰ ë° ë¶„ì„ì„ ìœ„í•œ ë¶„ì‚° ê²€ìƒ‰ ì—”ì§„
- **Kibana**: Elasticsearch ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ëŠ” ë„êµ¬
- **Kafka**: ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë©”ì‹œì§€ ë¸Œë¡œì»¤
- **Debezium**: ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½ ì‚¬í•­ì„ ê°ì§€í•˜ëŠ” CDC í”„ë ˆì„ì›Œí¬

## **3. í”„ë¡œì íŠ¸ ì„¤ì •**
> [!NOTE]
> - Docker : Kafka, Zookeeper, Kafka Connect, MySQL, Debezium, Elasticsearch, Kibana ì‹¤í–‰
> - Kafka & Zookeeper: ë©”ì‹œì§€ ë¸Œë¡œì»¤ ì—­í• 
> - Debezium: MySQLì˜ ë³€ê²½ ì‚¬í•­ì„ Kafkaë¡œ ì „ì†¡
> - Java ê¸°ë°˜ Kafka Consumer: CDC ì´ë²¤íŠ¸ ì²˜ë¦¬ í›„ Elasticsearch ì €ì¥
> - Kibana: Elasticsearch ë°ì´í„° ì‹œê°í™”

### **1) build.gradle** 
```groovy
// Kafka
implementation 'org.springframework.kafka:spring-kafka'
// Elasticsearch
implementation 'org.springframework.boot:spring-boot-starter-data-elasticsearch'
//Redis  
implementation 'org.springframework.boot:spring-boot-starter-data-redis'
```
### **2) application.yml**
```yaml
kafka:
  bootstrap-servers: localhost:29092
  consumer:
    group-id: post-consumer-group
    auto-offset-reset: earliest
    key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
```
### **3) ì»¤ìŠ¤í…€ ë„ì»¤ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì •**
```yml
#DockerFile. í”„ë¡œì íŠ¸ ë£¨íŠ¸ê²½ë¡œì— ìœ„ì¹˜ì‹œì¼°ìŒ
FROM confluentinc/cp-kafka-connect:latest  
  
# Confluent Hub CLIë¥¼ ì‚¬ìš©í•˜ì—¬ Elasticsearch ë° Debezium í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜  
RUN confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:latest \  
    && confluent-hub install --no-prompt debezium/debezium-connector-mysql:latest  
  
# Kafka Connect ì‹¤í–‰  
CMD ["bash", "-c", "echo 'Starting Kafka Connect'; /etc/confluent/docker/run"]
```
##### **ğŸ“Œ Dockerfileì„ ì‚¬ìš©í•˜ì—¬ custom-kafka-connectë¼ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±**
```bash
docker build -t custom-kafka-connect .
```

>[!NOTE]
> confluentinc/cp-kafka-connect ì´ë¯¸ì§€ëŠ” elastic sink connector í”ŒëŸ¬ê·¸ì¸ì„ í¬í•¨í•˜ì§€ ì•Šì•„ ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
> ì œê°€ ì°¾ì•„ë³¸ ê²°ê³¼ debezium connectorì™€ elastic search sink connector ëª¨ë‘ í¬í•¨í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì»¤ìŠ¤í…€ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í–ˆëŠ”ë° í˜¹ì‹œ ë‹¤ë¥¸ë°©ë²•ì„ ì°¾ê²Œë˜ë©´ ì—…ë°ì´íŠ¸ í•˜ê² ìŠµë‹ˆë‹¤.

### **4) Dockerë¡œ Kafka, Zookeeper, MySQL, Elasticsearch, Kibana ì„¤ì •**
```yaml
# Docker Compose íŒŒì¼ ì‘ì„± (docker-compose.yml)
services:  
  zookeeper:  
    image: quay.io/debezium/zookeeper:3.0  
    container_name: zookeeper  
    restart: always  
    ports:  
      - "2181:2181"  
    environment:  
      - ZOOKEEPER_CLIENT_PORT=2181  
  
  kafka:  
    image: confluentinc/cp-kafka:latest  
    container_name: kafka  
    restart: always  
    environment:  
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092  
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092  
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT  
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT  
      KAFKA_BROKER_ID: 1  
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1  
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"  #  ìë™ í† í”½ ìƒì„± í™œì„±í™”  
    ports:  
      - "9092:9092"  
      - "29092:29092"  
    depends_on:  
      - zookeeper  
  
  mysql:  
    image: mysql:8.0  
    container_name: mysql  
    restart: always  
    ports:  
      - "3306:3306"  
    environment:  
      MYSQL_ROOT_PASSWORD: root  
      MYSQL_USER: user  
      MYSQL_PASSWORD: password  
      MYSQL_DATABASE: mydb  
    volumes:  
      - mysql_data:/var/lib/mysql  
  
  redis:  
    image: "redis:latest"  
    container_name: redis  
    ports:  
      - "6379:6379"  
    restart: always  
  
  kafka-connect:  
    image: custom-kafka-connect  
    container_name: kafka-connect  
    restart: always  
    depends_on:  
      - kafka  
      - elasticsearch  
    ports:  
      - "8083:8083"  
    environment:  
      - CONNECT_BOOTSTRAP_SERVERS=kafka:9092  
      - CONNECT_GROUP_ID=connect-cluster  
      - CONNECT_CONFIG_STORAGE_TOPIC=connect-configs  
      - CONNECT_OFFSET_STORAGE_TOPIC=connect-offsets  
      - CONNECT_STATUS_STORAGE_TOPIC=connect-statuses  
      - CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1  
      - CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1  
      - CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1  
      - CONNECT_REST_ADVERTISED_HOST_NAME=kafka-connect  
      - CONNECT_PLUGIN_PATH=/usr/share/confluent-hub-components  
      - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter  CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter  
      - CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE=false  #JSON Converterì¼ ë•Œ í•„ìš”  
      - CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false  #JSON Converterì¼ ë•Œ í•„ìš”  
  
  elasticsearch:  
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.3  
    container_name: elasticsearch  
    restart: always  
    environment:  
      - discovery.type=single-node  
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"  
      - xpack.security.enabled=false  
    ports:  
      - "9200:9200"  
  
  kibana:  
    image: docker.elastic.co/kibana/kibana:8.5.3  
    container_name: kibana  
    depends_on:  
      - elasticsearch  
    ports:  
      - "5601:5601"  
  
volumes:  
  mysql_data:
```
##### **ğŸ“Œ Docker ëª…ë ¹ì–´ ì°¸ê³ **
```bash
# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker-compose up -d

# ì»¨í…Œì´ë„ˆ ì¤‘ì§€
docker-compose down

# íŠ¹ì • ì»¨í…Œì´ë„ˆë§Œ ì¬ì‹œì‘
docker-compose restart <container-name>

# ë„ì»¤ ì»¨í…Œì´ë„ˆ ìƒíƒœí™•ì¸
docker ps 
```
### **5) MySQL CDC ì„¤ì •**
```bash
 # mysql ì»¨í…Œì´ë„ˆì— ì ‘ì†
 docker exec -it mysql bash  
```
```sql
-- ì‚¬ìš©ì ìƒì„± ë° ê¶Œí•œ ë¶€ì—¬
CREATE USER 'debezium'@'%' IDENTIFIED BY 'dbz';
GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'debezium'@'%';
FLUSH PRIVILEGES;
```

>[!NOTE]
>Debezium MySQL ì»¤ë„¥í„° ì„¤ì • ì „ì— ì»¤ë„¥í„°ê°€ ë³€ê²½ì‚¬í•­ì„ ìº¡ì³í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì˜ê¶Œí•œì„ ë¶€ì—¬í•´ì•¼í•©ë‹ˆë‹¤.
### **6) Debezium MySQL ì»¤ë„¥í„° ì„¤ì •**
##### ğŸ“Œ ì»¤ë„¥í„° ë“±ë¡
- **Method**: `POST`
- **URL**: `http://localhost:8083/connectors/`
- **Content-Type**: `application/json`
##### ğŸ“ ìš”ì²­ ë³¸ë¬¸ (Request Body)
```json
{
  "name": "mydb-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "database.hostname": "mysql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",
    "topic.prefix": "mingle",
    "database.include.list": "mydb",
    "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
    "schema.history.internal.kafka.topic": "schemahistory.mydb",
    "include.schema.changes": "true",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "true"
  }
}
```
##### ğŸ“ Response Body(ì •ìƒìƒíƒœ)
```json
{
  "name": "mydb-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "database.hostname": "mysql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",
    "topic.prefix": "mingle",
    "database.include.list": "mydb",
    "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
    "schema.history.internal.kafka.topic": "schemahistory.mydb",
    "include.schema.changes": "true",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "true"
  },
  "tasks": [],
  "type": "source"
}
```
##### ğŸ“Œ  ì»¤ë„¥í„° ë“±ë¡ í›„ í™•ì¸
- **Method**: `GET`
- **URL** : `http://localhost:8083/connectors`
##### ğŸ“ Response Body(ì •ìƒìƒíƒœ)
``` json
[
	"mydb-connector"
]
```
##### ğŸ“Œ  ì»¤ë„¥í„° ìƒíƒœ í™•ì¸
- **Method**: `GET`
- **URL**: `http://localhost:8083/connectors/<ì»¤ë„¥í„°ëª…>/status`
##### ğŸ“ Response Body(ì •ìƒ ìƒíƒœ)
```json
{
  "name": "mydb-connector",
  "connector": {
    "state": "RUNNING",
    "worker_id": "kafka-connect:8083"
  },
  "tasks": [
    {
      "id": 0,
      "state": "RUNNING",
      "worker_id": "kafka-connect:8083"
    }
  ],
  "type": "source"
}
```
##### ğŸ“Œ  ì»¤ë„¥í„° ì‚­ì œ
- Method: `DELETE`
- URL: `http://localhost:8083/connectors/<ì»¤ë„¥í„°ëª…>
### **7) Elasticsearch Sink ì»¤ë„¥í„° ì„¤ì •**
##### ğŸ“Œ ì»¤ë„¥í„° ë“±ë¡
- **Method**: `POST`
- **URL**: `http://localhost:8083/connectors/`
- **Content-Type**: `application/json`
##### ğŸ“ ìš”ì²­ ë³¸ë¬¸ (Request Body)
```json
{
  "name": "elasticsearch-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "topics.regex": "mysql\\.mydb\\..*",
    "connection.url": "http://elasticsearch:9200",
    "type.name": "_doc",
    "key.ignore": "true",a
    "schema.ignore": "true"
  }
}
```
##### ğŸ“ Response Body(ì •ìƒìƒíƒœ)
```json
{
    "name": "elasticsearch-sink-connector",
    "config": {
        "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
        "topics.regex": "mysql\\.mydb\\..*",
        "connection.url": "http://elasticsearch:9200",
        "type.name": "_doc",
        "key.ignore": "true",
        "schema.ignore": "true",
        "name": "elasticsearch-sink-connector"
    },
    "tasks": [],
    "type": "sink"
}
```
##### ğŸ“Œ  ì»¤ë„¥í„° ë“±ë¡ í›„ í™•ì¸
- **Method**: `GET`
- **URL** : `http://localhost:8083/connectors`
##### ğŸ“ Response Body(ì •ìƒìƒíƒœ)
``` json
[
	"mydb-connector",
	"elasticsearch-sink-connector"
]
```
##### ğŸ“Œ  ì»¤ë„¥í„° ìƒíƒœ í™•ì¸
- **Method**: `GET`
- **URL**: `http://localhost:8083/connectors/<ì»¤ë„¥í„°ëª…>/status`
##### ğŸ“ Response Body(ì •ìƒ ìƒíƒœ)
```json
{
  "name": "mydb-connector",
  "connector": {
    "state": "RUNNING",
    "worker_id": "kafka-connect:8083"
  },
  "tasks": [
    {
      "id": 0,
      "state": "RUNNING",
      "worker_id": "kafka-connect:8083"
    }
  ],
  "type": "source"
}
```
##### ğŸ“Œ  ì„¤ì¹˜ëœ í”ŒëŸ¬ê·¸ì¸ í™•ì¸
- Method: `GET`
- URL: `http://localhost:8083/connector-plugins`
##### ğŸ“ Response Body(ì •ìƒ ìƒíƒœ)
```json
[
  {
    "class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "type": "sink",
    "version": "14.1.2"
  },
  {
    "class": "io.debezium.connector.mysql.MySqlConnector",
    "type": "source",
    "version": "2.4.2.Final"
  },
  {
    "class": "org.apache.kafka.connect.mirror.MirrorCheckpointConnector",
    "type": "source",
    "version": "7.9.0-ccs"
  },
  {
    "class": "org.apache.kafka.connect.mirror.MirrorHeartbeatConnector",
    "type": "source",
    "version": "7.9.0-ccs"
  },
  {
    "class": "org.apache.kafka.connect.mirror.MirrorSourceConnector",
    "type": "source",
    "version": "7.9.0-ccs"
  }
]
```
>[!NOTE]
> **ElasticsearchSinkConnector** ì™€ **debezium.connector** ê°€ ì¡´ì¬í•´ì•¼í•©ë‹ˆë‹¤!

---
## **4. KafkaConsumer.java (Redis Cache ì €ì¥)**
```java
@Slf4j  
@Service  
@RequiredArgsConstructor  
public class KafkaConsumer {  
    private final RedisService redisService;  
    private final PostService postService;  
    private final ObjectMapper objectMapper;  
  
    @KafkaListener(topics = "mingle.mydb.post", groupId = "post-consumer-group")  
    public void listen(ConsumerRecord<String, String> record) {  
        try {  
            JsonNode jsonNode = objectMapper.readTree(record.value());  
            JsonNode payload = jsonNode.get("payload");  
  
            if (payload == null || !payload.hasNonNull("op")) {  
                log.warn("Kafka ë©”ì‹œì§€ì— ìœ íš¨í•œ payloadê°€ ì—†ìŒ: {}", record.value());  
                return;  
            }  
  
            String operation = payload.get("op").asText();  
            JsonNode afterNode = payload.get("after");  
            JsonNode beforeNode = payload.get("before");  
  
            switch (operation) {  
                case "c":  
                    handleInsertEvent(afterNode);  
                    break;  
                case "u":  
                    handleUpdateEvent(beforeNode, afterNode);  
                    break;  
                case "d":  
                    handleDeleteEvent(beforeNode);  
                    break;  
                default:  
                    log.warn("ì§€ì›í•˜ì§€ ì•ŠëŠ” Kafka ì´ë²¤íŠ¸ ìœ í˜•: {}", operation);  
            }  
  
        } catch (Exception e) {  
            log.error("Kafka ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {}", e.getMessage(), e);  
        }  
    }  
  
    /**  
     * ìƒˆ ê²Œì‹œê¸€ì´ ìƒì„±ë  ë•Œ ì²˜ë¦¬  
     */  
    private void handleInsertEvent(JsonNode afterNode) {  
        if (afterNode == null || !afterNode.hasNonNull("post_id") || !afterNode.hasNonNull("author_id")) {  
            log.warn("INSERT ì´ë²¤íŠ¸ì—ì„œ post_id ë˜ëŠ” author_idê°€ ëˆ„ë½ë¨: {}", afterNode);  
            return;  
        }  
  
        long postId = afterNode.get("post_id").asLong();  
        long authorId = afterNode.get("author_id").asLong();  
        updateRedisCache(authorId, postId);  
        log.info(" [INSERT] ê²Œì‹œê¸€ ì¶”ê°€ë¨ - post_id: {}, author_id: {}", postId, authorId);  
    }  
  
    /**  
     * ê¸°ì¡´ ê²Œì‹œê¸€ì´ ìˆ˜ì •ë  ë•Œ ì²˜ë¦¬ (contentê°€ ë³€ê²½ëœ ê²½ìš°ë§Œ)  
     */    private void handleUpdateEvent(JsonNode beforeNode, JsonNode afterNode) {  
        if (beforeNode == null || afterNode == null ||  
                !beforeNode.hasNonNull("content") || !afterNode.hasNonNull("content")) {  
            log.warn("UPDATE ì´ë²¤íŠ¸ì—ì„œ content í•„ë“œê°€ ëˆ„ë½ë¨: before={}, after={}", beforeNode, afterNode);  
            return;  
        }  
  
        boolean isContentChanged = !beforeNode.get("content").asText().equals(afterNode.get("content").asText());  
        if (!isContentChanged) {  
            return;  
        }  
  
        long postId = afterNode.get("post_id").asLong();  
        long authorId = afterNode.get("author_id").asLong();  
        updateRedisCache(authorId, postId);  
        log.info("ğŸ”„ [UPDATE] ê²Œì‹œê¸€ ë‚´ìš© ë³€ê²½ - post_id: {}, author_id: {}", postId, authorId);  
    }  
  
    /**  
     * ê²Œì‹œê¸€ì´ ì‚­ì œë  ë•Œ ì²˜ë¦¬  
     */  
    private void handleDeleteEvent(JsonNode beforeNode) {  
        if (beforeNode == null || !beforeNode.hasNonNull("post_id") || !beforeNode.hasNonNull("author_id")) {  
            log.warn("DELETE ì´ë²¤íŠ¸ì—ì„œ post_id ë˜ëŠ” author_idê°€ ëˆ„ë½ë¨: {}", beforeNode);  
            return;  
        }  
  
        long deletedId = beforeNode.get("post_id").asLong();  
        long authorId = beforeNode.get("author_id").asLong();  
        deleteRedisCache(authorId, deletedId);  
        log.info(" [DELETE] ê²Œì‹œê¸€ ì‚­ì œë¨ - post_id: {}, author_id: {}", deletedId, authorId);  
    }  
  
    /**  
     * Redis ìºì‹œì— ê²Œì‹œê¸€ ì¶”ê°€  
     */  
    private void updateRedisCache(Long authorId, Long postId) {  
        Set<String> followers = redisService.getFollowers(authorId);  
        if (followers.isEmpty()) {  
            log.info(" [Redis] authorId {} ì— ëŒ€í•œ íŒ”ë¡œì›Œ ì—†ìŒ, ì—…ë°ì´íŠ¸ ìƒëµ", authorId);  
            return;  
        }  
        for (String follower : followers) {  
            redisService.updateCache(Long.valueOf(follower), postId);  
        }  
    }  
  
    /**  
     * Redis ìºì‹œì—ì„œ ê²Œì‹œê¸€ ì‚­ì œ  
     */  
    private void deleteRedisCache(Long authorId, Long postId) {  
        Set<String> followers = redisService.getFollowers(authorId);  
        if (followers.isEmpty()) {  
            log.info(" [Redis] authorId {} ì— ëŒ€í•œ íŒ”ë¡œì›Œ ì—†ìŒ, ì‚­ì œ ìƒëµ", authorId);  
            return;  
        }  
        for (String follower : followers) {  
            redisService.deleteCache(Long.valueOf(follower), postId);  
        }  
    }  
}
```

## **5. Kibanaì—ì„œ ë°ì´í„° í™•ì¸**

##### **1) Elasticsearch ì¸ë±ìŠ¤ í™•ì¸**
```bash
curl -X GET "http://localhost:9200/_cat/indices?v"
```

##### **2) Kibana ì ‘ì† í›„ í™•ì¸**
1. `http://localhost:5601`ë¡œ ì ‘ì†
2. `Stack Management > Index Patterns`ì—ì„œ `mingle.mydb.post` ì¸ë±ìŠ¤ ìƒì„±
3. `Discover`ì—ì„œ ë°ì´í„° ì¡°íšŒ ê°€ëŠ¥

---
## **6. ê²°ë¡ **
##### **âœ… MySQL CDC ë°ì´í„° íë¦„**
1. **MySQL Binlog** â†’ MySQLì—ì„œ ë³€ê²½ ì‚¬í•­ì´ ë°œìƒí•˜ë©´ **Binlog**ì— ê¸°ë¡ë¨
2. **Debezium CDC ê°ì§€** â†’ Debeziumì´ Binlogë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ë³€ê²½ ì‚¬í•­ì„ ê°ì§€
3. **Kafka ë¸Œë¡œì»¤ë¡œ ì „ì†¡** â†’ ê°ì§€ëœ ë³€ê²½ ë°ì´í„°ë¥¼ Kafka í† í”½ì— ê²Œì‹œ
4. **Kafka Consumer** â†’ Kafka Consumerê°€ ë©”ì‹œì§€ë¥¼ ì½ì–´ **Redisì— ìºì‹±**
5. **Elasticsearch Connector** â†’ afka Connectì˜ **Elasticsearch Sink**ê°€ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ì†Œë¹„í•˜ì—¬ **Elasticsearchì— ì €ì¥**

